{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from string import punctuation\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reding the txt files\n",
    "with open('Data/reviews.txt','r') as f:\n",
    "    reviews = f.read()\n",
    "with open('Data/labels.txt','r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  \n",
      "......\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:200])\n",
    "print('......')\n",
    "print(labels[:62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.lower()\n",
    "labels = labels.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Puntuations\n",
    "reviews = ''.join([c for c in reviews if c not in punctuation])\n",
    "\n",
    "print('\\n' in reviews)\n",
    "\n",
    "#creating a list of reviews\n",
    "reviews_split = reviews.split('\\n')\n",
    "reviews= ''.join(reviews_split)\n",
    "\n",
    "print('\\n' in reviews)\n",
    "\n",
    "# Creating a list of all the words in the file.\n",
    "words = reviews.split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Words: 74072\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Getting the words in the most frequent order\n",
    "c= Counter(words)\n",
    "vocab = sorted(c, key=c.get,reverse=True)\n",
    "#Creatng a vocab_to_int dictionary for future reference\n",
    "vocab_to_int = {word:i for i,word in enumerate(vocab,1)}\n",
    "print('Unique Words:',len(vocab_to_int))\n",
    "\n",
    "#Creating a list of indices for each word in each review\n",
    "review_ints = []\n",
    "for review in reviews_split:\n",
    "    review_ints.append([vocab_to_int[word] for word in review.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding the labels\n",
    "labels = labels.split('\\n')\n",
    "encoded_labels = [0 if i =='negative' else 1 for i in labels]\n",
    "encoded_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25001\n",
      "25001\n"
     ]
    }
   ],
   "source": [
    "print(len(review_ints))\n",
    "print(len(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of reviews before the processing:25001\n",
      "no. of reviews after the processing:25000\n"
     ]
    }
   ],
   "source": [
    "# Removing Zero length reviews\n",
    "print(f'no. of reviews before the processing:{len(review_ints)}')\n",
    "\n",
    "zero_len_idx = [i for i,review in enumerate(review_ints) if len(review) == 0]\n",
    "review_ints = [review_ints[i] for i in range(len(review_ints)) if i not in zero_len_idx]\n",
    "encoded_labels = [encoded_labels[i] for i in range(len(encoded_labels)) if i not in zero_len_idx]\n",
    "\n",
    "print(f'no. of reviews after the processing:{len(review_ints)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the reviews so that, all reviews have same lengths.\n",
    "def pad_features(reviews_ints, seq_length=200):\n",
    "    features= np.zeros((len(reviews_ints),seq_length), dtype=int)\n",
    "    for i,review in enumerate(reviews_ints):\n",
    "        features[i,-len(review):] = np.array(review)[:seq_length]\n",
    "    \n",
    "    # for review in review_ints:   \n",
    "    #     if len(review)<=seq_length:\n",
    "    #         temp = [0]*seq_length\n",
    "    #         temp[seq_length-len(review):] = review\n",
    "    #         features.append(temp)\n",
    "    #     elif len(review) > seq_length:\n",
    "    #         features.append(review[:seq_length])\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 0, 1, 2, 3]), array([1, 2, 3, 4, 5])]\n"
     ]
    }
   ],
   "source": [
    "test_review_ints = [[1,2,3],[1,2,3,4,5,6,7,8,9,10]]\n",
    "temp_seq_len = 5\n",
    "temp_features = pad_features(test_review_ints,temp_seq_len)\n",
    "print([i for i in temp_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 and <class 'list'>\n",
      "25000 and <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "seq_len=250\n",
    "\n",
    "features = pad_features(reviews_ints=review_ints,seq_length=seq_len)\n",
    "print(f'{len(review_ints)} and {type(review_ints)}')\n",
    "print(f'{len(features)} and {type(features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac =0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x,rem_x = features[:split_idx],features[split_idx:]\n",
    "train_y,rem_y = encoded_labels[:split_idx],encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(rem_x)*0.5)\n",
    "valid_x, test_x = features[:test_idx],features[test_idx:]\n",
    "valid_y,test_y = encoded_labels[:test_idx],encoded_labels[test_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TensorDataset\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x),torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x),torch.from_numpy(test_y))\n",
    "\n",
    "#Creating Dataloader\n",
    "train_loader = DataLoader(train_data,batch_size=64,shuffle=True,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data,batch_size=64,shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data,batch_size=64,shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample feature shape:torch.Size([64, 250])\n",
      "Sample label shape:torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(valid_loader)\n",
    "sample_x,sample_y = dataiter.next()\n",
    "\n",
    "print(f'Sample feature shape:{sample_x.shape}')\n",
    "print(f'Sample label shape:{sample_y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_analysis(nn.Module, ):\n",
    "    def __init__(self,n_vocab,embed_dim,hidden_size,num_layers,output_size,drop_prob = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embed = nn.Embedding(n_vocab,embedding_dim=embed_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim, hidden_size, num_layers, \n",
    "            dropout=drop_prob, batch_first = True\n",
    "            )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size,output_size)\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embedding = self.embed(x)\n",
    "        lstm_out, hidden = self.lstm(embedding,hidden)\n",
    "        out = torch.sigmoid(self.fc(lstm_out))\n",
    "        \n",
    "        out = out.view(batch_size,-1)\n",
    "        out = out[:,-1]\n",
    "\n",
    "        return out,hidden\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment_analysis(\n",
       "  (embed): Embedding(74073, 500)\n",
       "  (lstm): LSTM(500, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 500\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "model = Sentiment_analysis(\n",
    "    n_vocab = vocab_size,embed_dim = embedding_dim,\n",
    "    hidden_size = hidden_dim,num_layers= n_layers,output_size = output_size\n",
    "    )\n",
    "\n",
    "model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1   Step:100    Loss:0.6153   val_loss:0.5130\n",
      "Epoch:1   Step:200    Loss:0.4251   val_loss:0.3951\n",
      "Epoch:1   Step:300    Loss:0.5561   val_loss:0.4794\n",
      "Epoch:2   Step:400    Loss:0.3258   val_loss:0.3109\n",
      "Epoch:2   Step:500    Loss:0.4104   val_loss:0.2452\n",
      "Epoch:2   Step:600    Loss:0.2872   val_loss:0.2127\n",
      "Epoch:3   Step:700    Loss:0.1314   val_loss:0.1653\n",
      "Epoch:3   Step:800    Loss:0.3239   val_loss:0.1666\n",
      "Epoch:3   Step:900    Loss:0.2300   val_loss:0.1647\n",
      "Epoch:4   Step:1000    Loss:0.1760   val_loss:0.1223\n",
      "Epoch:4   Step:1100    Loss:0.0921   val_loss:0.1484\n",
      "Epoch:4   Step:1200    Loss:0.1410   val_loss:0.1462\n",
      "Epoch:5   Step:1300    Loss:0.1160   val_loss:0.0852\n",
      "Epoch:5   Step:1400    Loss:0.1161   val_loss:0.1114\n",
      "Epoch:5   Step:1500    Loss:0.1599   val_loss:0.0994\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "counter = 0\n",
    "clip = 5\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    h = model.init_hidden(batch_size=64)\n",
    "\n",
    "    for input,label in train_loader:\n",
    "        counter+=1\n",
    "\n",
    "        model.train()\n",
    "        input,label = input.cuda(),label.cuda()\n",
    "        label = label.float()\n",
    "        h = tuple([each.data for each in h]) # Removing past references\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output,h = model(input,h)\n",
    "\n",
    "        loss = criterion(output.squeeze(),label)\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size=64)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for val_inputs,val_label in valid_loader:\n",
    "                \n",
    "                val_inputs,val_label = val_inputs.cuda(),val_label.cuda()\n",
    "                val_label = val_label.float()\n",
    "                val_h = tuple([each.data for each in val_h]) # Removing past references\n",
    "                try:\n",
    "                    val_output,val_h = model(val_inputs,val_h)\n",
    "                    \n",
    "\n",
    "                    val_loss = criterion(val_output.squeeze(),val_label)\n",
    "                    val_losses.append(val_loss.item())\n",
    "                except:\n",
    "                    print(f'Validation shape{val_output.shape}')\n",
    "                    print(f'Validation shape{val_h.shape}')\n",
    "\n",
    "\n",
    "            model.train()\n",
    "            print(f'Epoch:{epoch+1}   Step:{counter}    Loss:{loss.item():.4f}   val_loss:{np.mean(val_losses):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.211\n",
      "Test accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "train_on_gpu=True\n",
    "# init hidden state\n",
    "h = model.init_hidden(batch_size = 64)\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "    # get predicted outputs\n",
    "    output, h = model(inputs, h)\n",
    "\n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment_analysis(\n",
       "  (embed): Embedding(74073, 500)\n",
       "  (lstm): LSTM(500, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)+1\n",
    "output_size = 1\n",
    "embedding_dim = 500\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "model = Sentiment_analysis(\n",
    "    n_vocab = vocab_size,embed_dim = embedding_dim,\n",
    "    hidden_size = hidden_dim,num_layers= n_layers,output_size = output_size\n",
    "    )\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of some reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review_pos = 'What a wonderful experience this was! I was on the edge of my seat when I watched this movie. I still recall all the scenes. When I watched the intermission, I got goosebumps. I would definitely watch this movie again'\n",
    "test_review_neg = 'This is the worst movie that anyone could possibly make. I still cannot understand why the producer invested so much in this movie. The recently released low budget movie was better than this one. I probably would never watch this one again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "\n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "\n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "\n",
    "    batch_size = feature_tensor.size(0)\n",
    "\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "\n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "\n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.977527\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "predict(model,test_review_pos,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.004393\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "predict(model,test_review_neg,250)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0f18ce3f39c1e5ac77cd0f736ad77d935cb92d7a25fb751628ed8f5364e312d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
